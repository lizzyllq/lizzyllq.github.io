<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="BasicKL$D_{KL}(p(x)||q(x)) = \int p(x) log \frac{p(x)}{q(x)} dx = E_{x::p(x)} (log \frac{p(x)}{q(x)})$ Jensen 不等式 设f是定义域为实数的函数，如果对于所有的实数x。如果对于所有的实数x，f(x)的二次导数大于等于0，那么f是凸函数。当x是向量时，如果其hessian矩阵H是半正定的，那么">
<meta property="og:type" content="article">
<meta property="og:title" content="Generative models">
<meta property="og:url" content="http://yoursite.com/2019/07/01/generative-models/index.html">
<meta property="og:site_name" content="Forward">
<meta property="og:description" content="BasicKL$D_{KL}(p(x)||q(x)) = \int p(x) log \frac{p(x)}{q(x)} dx = E_{x::p(x)} (log \frac{p(x)}{q(x)})$ Jensen 不等式 设f是定义域为实数的函数，如果对于所有的实数x。如果对于所有的实数x，f(x)的二次导数大于等于0，那么f是凸函数。当x是向量时，如果其hessian矩阵H是半正定的，那么">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png">
<meta property="og:updated_time" content="2019-07-01T09:19:36.330Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generative models">
<meta name="twitter:description" content="BasicKL$D_{KL}(p(x)||q(x)) = \int p(x) log \frac{p(x)}{q(x)} dx = E_{x::p(x)} (log \frac{p(x)}{q(x)})$ Jensen 不等式 设f是定义域为实数的函数，如果对于所有的实数x。如果对于所有的实数x，f(x)的二次导数大于等于0，那么f是凸函数。当x是向量时，如果其hessian矩阵H是半正定的，那么">
<meta name="twitter:image" content="https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/01/generative-models/">





  <title>Generative models | Forward</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Forward</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/01/generative-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lizzy llq">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Forward">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Generative models</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-01T16:49:30+08:00">
                2019-07-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><h2 id="KL"><a href="#KL" class="headerlink" title="KL"></a>KL</h2><p>$D_{KL}(p(x)||q(x)) = \int p(x) log \frac{p(x)}{q(x)} dx = E_{x::p(x)} (log \frac{p(x)}{q(x)})$</p>
<h2 id="Jensen-不等式"><a href="#Jensen-不等式" class="headerlink" title="Jensen 不等式"></a>Jensen 不等式</h2><blockquote>
<p>设f是定义域为实数的函数，如果对于所有的实数x。如果对于所有的实数x，f(x)的二次导数大于等于0，那么f是凸函数。当<strong>x</strong>是向量时，如果其hessian矩阵H是半正定的，那么f是凸函数。如果只大于0，不等于0，那么称f是严格凸函数。</p>
</blockquote>
<p><strong>Jensen不等式</strong>：</p>
<p>如果f是凸函数，X是随机变量，那么：E[f(X)]&gt;=f(E[X])</p>
<p>特别地，如果f是严格凸函数，当且仅当X是常量时，上式取等号。</p>
<h2 id="discretized-distribution"><a href="#discretized-distribution" class="headerlink" title="discretized distribution"></a>discretized distribution</h2><ul>
<li>A <a href="https://en.wikipedia.org/wiki/Probability_distribution" target="_blank" rel="noopener">discrete probability distribution</a>is a probability distribution characterized by a  probability mass function (PMF).</li>
<li>But wait … since a discrete probability  distribution can’t be differentiated, how can we use it directly in neural  networks?</li>
<li>Only continuous probability distribution can be differentiated.</li>
<li>So, we need a discretized version of a continuous probability distribution.</li>
</ul>
<hr>
<ul>
<li><p>Given a distribution,</p>
</li>
<li><ul>
<li>$P(. | θ)$ is the probability density function       (PDF)</li>
<li>$F(. | θ)$ is the cumulative density function       (CDF)</li>
<li>$\theta$ is the parameter set.</li>
</ul>
</li>
<li><p>Given a set $X= \{x^0, x^1, …, x^{m} \}$ where $x^i \in ℝ$, the probability mass function (PMF) is</p>
</li>
<li><ul>
<li>$P(x^i | θ) = F(x^i + Δ | θ) - F(x^i - Δ | θ)$</li>
</ul>
</li>
<li><p>It’s easy to compute the gradients of a PMF.</p>
</li>
<li><ul>
<li><p>E.g.  Logistic distribution</p>
</li>
<li><ul>
<li>PDF : $ \frac{1}{s}\sigma(\frac{x-\mu}{s}) (1-\sigma(\frac{x-\mu}{s}))$, where $\sigma(x) = \frac{1}{1+exp(-x)}$</li>
<li>CDF : $\sigma(\frac{x-\mu}{s}) $</li>
</ul>
</li>
<li><ul>
<li>A discretized version PMF: $\sigma(\frac{x-\mu +  Δ}{s})  - \sigma(\frac{x-\mu -  Δ}{s}) $</li>
<li>使用logistic分布的原因是其discretized PDF的函数形式简单（两个sigmoid相减），计算代价低)</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>What is “mixture distribution”?</p>
</li>
<li><ul>
<li>A variable may have multiple  modes, so a single distribution is hard to model such a variable.</li>
<li>$p(x|\theta) = \sum_{i=1}^K w_ip_i(x|\theta_i)$  Where K is the number of components, θ is the parameter set.</li>
</ul>
</li>
<li><p>A softmax distribution can do that task as  well, why don’t use it?</p>
</li>
<li><ul>
<li><p>When the number of the valid dots is big, using softmax is inefficient.</p>
</li>
<li><ul>
<li>256-way softmax is good, but the 65536-way is a disaster.</li>
</ul>
</li>
<li><p>Softmax can’t utilize the inherent structure of continuous number</p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>How to use it in training?</p>
</li>
<li><ul>
<li>Use the discretized version to  calculate the likelihoods. It’s easy to back propagate.</li>
<li>Note: be careful of the numerical  problems.</li>
</ul>
</li>
<li><p>How to use it in generating?</p>
</li>
<li><ul>
<li>Use the continuous version of the distribution.</li>
<li>Use re-parametrize trick.</li>
</ul>
</li>
</ul>
<h1 id="参数估计：（分布已知，参数未知）"><a href="#参数估计：（分布已知，参数未知）" class="headerlink" title="参数估计：（分布已知，参数未知）"></a>参数估计：（分布已知，参数未知）</h1><ul>
<li>最大似然估计：使得样本发生的概率$P(Y|\theta)$最大的参数$\theta$即为所估计的参数$\hat{\theta} $,  (这里隐含的假设是所有$\theta$的先验概率均相等)</li>
<li>贝叶斯估计： 使$P(\theta)P(Y|\theta)$最大的参数$\theta$即为所估计的参数$\hat{\theta} $</li>
</ul>
<p><strong>求最大似然函数估计值的一般步骤：</strong></p>
<p>（1）写出似然函数；</p>
<p>（2）对似然函数取对数，并整理；</p>
<p>（3）对参数求偏导，令导数为0，得到似然方程；</p>
<p>（4）解似然方程，若有解析解，则得到的参数即为所求； 若无解析解，如神经网络，可通过梯度下降逼近。</p>
<h1 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h1><p><a href="https://blog.csdn.net/zouxy09/article/details/8537620" target="_blank" rel="noopener">https://blog.csdn.net/zouxy09/article/details/8537620</a></p>
<ul>
<li>优化目标：maximize $log\ p(X)$</li>
</ul>
<p>$log\ p(X;\theta) = \sum_i log \  p(x^{(i)};\theta) = \sum_i log \sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta)$       ($x^{(i)}$是常数，$\theta$和$z^{(i)}$是变量)</p>
<p>​                   $=\sum_i log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)},z^{(i)};\theta)}{Q^i(z_{(i)})} $</p>
<p>​                    $&gt;=\sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta)}{Q^i(z_{(i)})} $  （Jensen不等式）和的对数-&gt;对数的和，便于求导</p>
<p>初始化分布参数$\theta$</p>
<p>E:  根据参数初始值或上一次迭代的模型参数来计算出后验概率 $Q_i(z^{(i)})  = p(z^{(i)}|x^{(i)} ;\theta)$</p>
<p>M:  将似然函数最大化以获得新的参数值  $\theta := argmax_{\theta}\sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta)}{Q^i(z_{(i)})} $</p>
<h1 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h1><h2 id="Problem-Senario"><a href="#Problem-Senario" class="headerlink" title="Problem Senario"></a>Problem Senario</h2><blockquote>
<p>Let us consider some dataset $X = \{x^(i) \}_{ i=1}^ N$ consisting of $N$ i.i.d. samples of some continuous or discrete variable $x$. We assume that the data are generated by some random process, involving an unobserved continuous random variable $z$. The process consists of two steps: (1) a value $z^(i)$ is generated from some prior distribution $p_{θ^∗} (z)$; (2) a value $x^(i)$ is generated from some conditional distribution $p_{θ^∗} (x|z)$. We assume that the prior $p_{θ^∗} (z)$ and likelihood $p_{θ^∗} (x|z)$ come from parametric families of distributions $p_{θ} (z)$ and $p_{θ} (x|z)$, and that their PDFs are differentiable almost everywhere w.r.t. both θ and z. Unfortunately, a lot of this process is hidden from our view: the true parameters θ ∗ as well as the values of the latent variables z (i) are unknown to us.</p>
<auto-encoding variational bayes>



</auto-encoding></blockquote>
<ul>
<li>Given a dataset $X = {x^1,x^2,x^3…x^N}$</li>
<li>理想优化目标：maximize $log\ p(X)$</li>
</ul>
<blockquote>
<p>but in general this marginal likelihood is intractable to compute or differentiate directly for ﬂexible generative models, e.g. when components of the generative model are parameterized by neural networks.</p>
<improved variational inference with inverse autoregressive flow>

</improved></blockquote>
<p>Choose prior p(z) to be simple, e.g. Gaussian.</p>
<p>Conditional p(x|z) is complex (generates image) =&gt; represent with neural network</p>
<ul>
<li>Data likelihood $p_{\theta}(x)= \int p_{\theta}(z) p_{\theta}(x|z)$  (Intractible to compute $p_{\theta}(x|z)$ for every z, 无法直接用最大似然)</li>
<li>Posterior density also intractable $p_{\theta}(z|x) = p_{\theta}(x|z)p_{\theta}(z)/p_{\theta}(x)$  （Intractable data likelihood 无法采用EM算法）</li>
</ul>
<p>Solution: In addition to decoder network modeling $p_θ (x|z)$, define additional encoder network $q_ɸ (z|x) $that approximates $p_θ (z|x)$</p>
<p>$log \ p_{\theta}(x^{(i)}) = E_{z^{(i)}::q_{\phi}(z^{(i)}|x^{(i)})}[log \ p_{\theta}(x^{(i)})]$     ($ p_{\theta}(x^{(i)}$ does not depend on z)</p>
<p>​                     $=  E_{z^{(i)}}[log \ \frac{p_{\theta}(z^{(i)})p_{\theta}(x^{(i)}|z^{(i)})} {p_{\theta}(z^{(i)}|x^{(i)})}] $  (bayes)</p>
<p>​                     $ = E_{z^{(i)}}[log \ \frac{p_{\theta}(z^{(i)})p_{\theta}(x^{(i)}|z^{(i)})} {p_{\theta}(z^{(i)}|x^{(i)})}   \frac {q_{\phi}(z^{(i)}|x^{(i)})} {q_{\phi}(z^{(i)}|x^{(i)})}] $</p>
<p>​                      $=  E_{z^{(i)}}[log \ p_{\theta}(x^{(i)}|z^{(i)}) ]  - E_{z^{(i)}}[log \ \frac{q_{\phi}(z^{(i)}|x^{(i)})} {p_{\theta}(z^{(i)}}] +E_{z^{(i)}}[log \ \frac{q_{\phi}(z^{(i)}|x^{(i)})} {p_{\theta}(z^{(i)}|x^{(i)})}]  $</p>
<p>​                      $=  E_{z^{(i)}}[log \ p_{\theta}(x^{(i)}|z^{(i)}) ]  -D_{KL}({q_{\phi}(z^{(i)}|x^{(i)})}|| {p_{\theta}(z^{(i)}})+ D_{KL}({q_{\phi}(z^{(i)}|x^{(i)})} ||{p_{\theta}(z^{(i)}|x^{(i)})}) $</p>
<p>​                          <strong>reconstruction loss</strong>                <strong>KL loss </strong>                        $p_{\theta}(z^{(i)}|x^{(i)}) untractable, KL&gt;=0$</p>
<p>​                         <strong>Reparameterization</strong></p>
<blockquote>
 <cs231n _2017_lecture13>

</cs231n></blockquote>
<p>Pros:</p>
<ul>
<li><p>Principled approach to generative models</p>
</li>
<li><p>Allows inference of q(z|x), can be useful feature representation for other tasks</p>
</li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Maximizes lower bound of likelihood: okay, but not as good evaluation as PixelRNN/PixelCNN</p>
</li>
<li><p>Samples blurrier and lower quality compared to state-of-the-art (GANs)</p>
</li>
</ul>
<h1 id="Flow"><a href="#Flow" class="headerlink" title="Flow"></a>Flow</h1><ul>
<li>Explicit density   显示的对$p(x)$进行建模</li>
<li>Tractable density</li>
</ul>
<p>$x$~$p(x)$, $z$~$\pi(z)$ , $x = f(z)$   The function $f$ is invertible</p>
<p>=&gt; $p(x)=\pi(z)|det(\frac{dz}{dx})|$</p>
<p><img src="https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png" alt="Normalizing flow"></p>
<p>=&gt; $p_i(z_i) = p_{i-1}(z_{i-1}) |det (\frac{dz_{i-1}}{dz_i})| =  p_{i-1}(z_{i-1}) |det ((\frac{dz_i}{dz_{i-1}})^{-1})| = p_{i-1}(z_{i-1}) |det (\frac{dz_i}{dz_{i-1}})|^{-1}$    </p>
<blockquote>
<p> because  $det(AB)=det(A)det(B)$   ,   $1 = det(AA^{-1})) = det(A)det(A^{-1})$  </p>
</blockquote>
<p>=&gt; $log \ p_i(z_i)  = log \ p_{i-1}(z_{i-1})  - log |det (\frac{dz_i}{dz_{i-1}})|$</p>
<p>=&gt; $log \ p_i(z_i)  = log \ p_{0}(z_{0})  - \sum_{i=1}^{K} log |det (\frac{dz_i}{dz_{i-1}})|$</p>
<p>Required by the computation in the equation, a transformation function fifi should satisfy two properties:</p>
<ol>
<li>It is easily invertible.  (easy to generate data)</li>
<li>Its Jacobian determinant is easy to compute.  (easy to compute loss)</li>
</ol>
<h2 id="autoregressive"><a href="#autoregressive" class="headerlink" title="autoregressive"></a>autoregressive</h2><blockquote>
<p>Examples of such functions are autoregressive neural density estimators such as RNNs, MADE (Germain et al., 2015), PixelCNN (van den Oord et al., 2016b) or WaveNet (van den Oord et al., 2016a) models.</p>
<improved variational inference with inverse autoregressive flow>

</improved></blockquote>
<h3 id="PixelRNN-PixelCNN-Wavenet"><a href="#PixelRNN-PixelCNN-Wavenet" class="headerlink" title="PixelRNN   /   PixelCNN  /  Wavenet"></a>PixelRNN   /   PixelCNN  /  Wavenet</h3><p><strong>define tractable density function</strong></p>
<p>优化目标： maximize $p(x) = \prod_{i=1}^n \ p(x_i|x_1,x_2,…,x_{i-1})$</p>
<ul>
<li>用神经网络(RNN/CNN)描述$p(x_i|x_1,x_2,…,x_{i-1})$</li>
<li>Softmax 计算概率(神经网络输出N个值,经过softmax归一化为N个类别的概率值)  </li>
<li>cross entropy计算loss</li>
<li>argmax采样 | 按照softmax probability随机采样</li>
</ul>
<p>pros</p>
<ul>
<li>Can explicitly compute likelihood p(x)</li>
<li>Explicit likelihood of training data gives good evaluation metric</li>
<li>Good samples</li>
</ul>
<p>cons</p>
<ul>
<li>Sequential generation =&gt; slow (CNN训练可并行，但生成还是序列化的)</li>
</ul>
<hr>
<p>改进的loss: </p>
<p>根据data的连续或离散选取连续分布或者离散化的分布</p>
<ul>
<li>Gaussian loss   （神经网络预测mean &amp; scale）</li>
</ul>
<p>​         $x_t=z_t*\sigma(x_{&lt;t})+\mu(x_{&lt;t})$   其中$\sigma()$和$\mu()$均为神经网络,    $z_t$服从高斯分布</p>
<p>​         神经网络输出2个值：$\mu   ,  log \ \sigma$       ( $log \ \sigma$ 可正可负)</p>
<ul>
<li><p>Discretized logistic loss</p>
<p>A mixture of logistics (MoL) is a set of M  Logistic distributions, each with its own $\mu$ and $\sigma$ and $w$ which will determine the probability of a distribution to be picked.</p>
<p>神经网络输出3M个值</p>
</li>
</ul>
<h2 id="Inverse-Autoregressive-Flow"><a href="#Inverse-Autoregressive-Flow" class="headerlink" title="Inverse Autoregressive Flow"></a>Inverse Autoregressive Flow</h2><h3 id="parallel-wavenet"><a href="#parallel-wavenet" class="headerlink" title="parallel wavenet"></a>parallel wavenet</h3>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/21/Git教程/" rel="next" title="Git教程">
                <i class="fa fa-chevron-left"></i> Git教程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">lizzy llq</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic"><span class="nav-number">1.</span> <span class="nav-text">Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#KL"><span class="nav-number">1.1.</span> <span class="nav-text">KL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Jensen-不等式"><span class="nav-number">1.2.</span> <span class="nav-text">Jensen 不等式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discretized-distribution"><span class="nav-number">1.3.</span> <span class="nav-text">discretized distribution</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参数估计：（分布已知，参数未知）"><span class="nav-number">2.</span> <span class="nav-text">参数估计：（分布已知，参数未知）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EM"><span class="nav-number">3.</span> <span class="nav-text">EM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VAE"><span class="nav-number">4.</span> <span class="nav-text">VAE</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem-Senario"><span class="nav-number">4.1.</span> <span class="nav-text">Problem Senario</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flow"><span class="nav-number">5.</span> <span class="nav-text">Flow</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#autoregressive"><span class="nav-number">5.1.</span> <span class="nav-text">autoregressive</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PixelRNN-PixelCNN-Wavenet"><span class="nav-number">5.1.1.</span> <span class="nav-text">PixelRNN   /   PixelCNN  /  Wavenet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inverse-Autoregressive-Flow"><span class="nav-number">5.2.</span> <span class="nav-text">Inverse Autoregressive Flow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#parallel-wavenet"><span class="nav-number">5.2.1.</span> <span class="nav-text">parallel wavenet</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lizzy llq</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
